{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "GOOGLE_API_KEY = \"your google api key\"\n",
    "ChatGoogleGenerativeAI.google_api_key = GOOGLE_API_KEY\n",
    "\n",
    "PINECONE_API_KEY=\"your pinecone api key\"\n",
    "index_name=\"ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore as lang_pinecone\n",
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Q/A Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lahore is the capital city of the Pakistani province of Punjab.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "prompt_str=\"\"\"\n",
    "Answer the user question briefly.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "question_fetcher=itemgetter(\"question\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.3,\n",
    "                             api_key=GOOGLE_API_KEY)\n",
    "\n",
    "\n",
    "chain = question_fetcher| prompt | llm | StrOutputParser()\n",
    "query = \"what is lahore\"  # Question here\n",
    "response = chain.invoke({\"question\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q/A Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asif Computer\\AppData\\Local\\Temp\\ipykernel_5260\\3555293365.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = PyPDFLoader(\"AI_Engineer_Roadmap.pdf\")\n",
    "pages = loaders.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents into vectors using LangPinecone\n",
    "vector = lang_pinecone.from_documents(\n",
    "    splits,             # List of Document objects to be converted into vectors\n",
    "    embed_model,             # Embedding model used for generating vector representations\n",
    "    index_name=index_name    # Name of the Pinecone index where vectors will be stored\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str=\"\"\"\n",
    "Answer the user question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "_prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "num_chunks=3\n",
    "retriever = vector.as_retriever(search_type=\"similarity\",\n",
    "                                        search_kwargs={\"k\": num_chunks})\n",
    "chat_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.3,\n",
    "                             api_key=GOOGLE_API_KEY)\n",
    "query_fetcher= itemgetter(\"question\")\n",
    "setup={\"question\":query_fetcher,\"context\":query_fetcher | retriever | format_docs}\n",
    "_chain = (setup |_prompt | chat_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"classification?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=_chain.invoke({\"question\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Text classification: Na√Øve Bayes', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-7a8616ed-77c2-4b2e-95ab-a539438c87fd-0', usage_metadata={'input_tokens': 1051, 'output_tokens': 7, 'total_tokens': 1058})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AI roadmap includes learning technical skills (or tool skills) and soft (or core) skills. The technical skills include coding and math, while the soft skills include research and protection from scams. The total duration of the roadmap is 8 months, with 4 hours of study every day.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or artificial intelligence, is the simulation of human intelligence processes by machines, especially computer systems. It involves tasks such as learning, reasoning, and problem-solving. AI is used in various fields, including natural language processing, image recognition, and robotics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_str=\"\"\"\n",
    "Answer the user question briefly.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "conversation_history: {chat_history}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "question_fetcher=itemgetter(\"question\")\n",
    "history_fetcher=itemgetter(\"chat_history\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.3,\n",
    "                             api_key=GOOGLE_API_KEY)\n",
    "setup={\"question\":question_fetcher,\"chat_history\":history_fetcher}\n",
    "chain = setup|prompt | llm | StrOutputParser()\n",
    "query = \"tell me about ai\"\n",
    "response = chain.invoke({\"question\": query,\"chat_history\":\"\\n\".join(str(history))})\n",
    "print(response)\n",
    "query=\"user_question:\"+query\n",
    "response=\"ai_response:\"+response\n",
    "history.append((query, response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user_question:tell me about lahore',\n",
       "  'ai_response:Lahore is the capital of the Pakistani province of Punjab and is the second-largest city in Pakistan after Karachi. It is located in the northeast of the country, near the border with India. Lahore is a major cultural, historical, and economic center of Pakistan. The city is home to many historical monuments, including the Lahore Fort, the Badshahi Mosque, and the Shalimar Gardens. Lahore is also a major center of education, with several universities and colleges located in the city.'),\n",
       " ('user_question:tell me about ai',\n",
       "  'ai_response:AI, or artificial intelligence, is the simulation of human intelligence processes by machines, especially computer systems. It involves tasks such as learning, reasoning, and problem-solving. AI is used in various fields, including natural language processing, image recognition, and robotics.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
