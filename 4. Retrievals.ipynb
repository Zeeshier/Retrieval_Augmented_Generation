{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_core.documents import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asif Computer\\AppData\\Local\\Temp\\ipykernel_8284\\2987287500.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
      "c:\\Users\\Asif Computer\\anaconda3\\envs\\conda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model with BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document using PyPDFLoader\n",
    "loaders = PyPDFLoader(\"AI_Engineer_Roadmap.pdf\")\n",
    "\n",
    "# Extract pages from the loaded PDF\n",
    "pages = loaders.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'AI_Engineer_Roadmap.pdf', 'page': 9}, page_content=' \\n   \\ncodebasics.io  \\n \\n10 \\no Track B (Affordable Fees):  \\n▪ Included in  the above Master Machine Learning for Data Science & AI  \\n \\nWeek 23, 24: Machine Learning Projects  with Deployment       \\n \\n• You need to finish two end to end ML projects. One on Regression , the other on \\nClassification  \\n• Regression Project: Bangalore property price prediction  \\no YouTube playlist link: https://bit.ly/3ivycWr  \\no Project covers following  \\n▪ Data cleaning  \\n▪ Feature engineering  \\n▪ Model building and hyper parameter tuning  \\n▪ Write flask server as a web backend  \\n▪ Building website for price prediction  \\n▪ Deployment to AWS  \\n• Classification Project: Sports celebrity image classification  \\no YouTube playlist link: https://bit.ly/3ioaMSU  \\no Project covers following  \\n▪ Data collection and data cleaning  \\n▪ Feature engineering and model training  \\n▪ Flask server as a web backend  \\n▪ Building website and deployment  \\n• ATS Resume Preparation  \\no Resumes are dying but not dead yet. Focus more on online presence.  \\no Here is the resume tips video along with some templates you can use for your \\ndata analyst resume: https://www.youtube.com/watch?v=buQSI8NLOMw  \\no Use this checklist to ensure you have the right ATS Resume:  Check here.  \\n \\n \\n• Portfolio Building Resources:  \\nYou need a portfolio website in 2024. You can build your portfolio by using these free \\nresources.  \\n• GitHub  \\no Upload your projects with code on github and using github.io create a \\nportfolio website  \\no Sample portfolio website: http://rajag0pal.github.io/  \\n ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store processed document chunks\n",
    "doc_list = []\n",
    "\n",
    "# Iterate over each page in the extracted pages\n",
    "for page in pages:\n",
    "    # Split the page content into smaller chunks\n",
    "    pg_split = text_splitter.split_text(page.page_content)\n",
    "\n",
    "    # Iterate over each chunk and create Document objects\n",
    "    for pg_sub_split in pg_split:\n",
    "        # Metadata for each chunk, including source and page number\n",
    "        metadata = {\"source\": \"AI policy\", \"page_no\": page.metadata[\"page\"] + 1}\n",
    "\n",
    "        # Create a Document object with content and metadata\n",
    "        doc_string = Document(page_content=pg_sub_split, metadata=metadata)\n",
    "\n",
    "        # Append the Document object to the list\n",
    "        doc_list.append(doc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_url = \"\"\n",
    "qdrant_key = \"\"\n",
    "collection_name = \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
