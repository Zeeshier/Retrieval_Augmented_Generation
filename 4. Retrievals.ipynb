{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from langchain_core.documents import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asif Computer\\AppData\\Local\\Temp\\ipykernel_8284\\2987287500.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
      "c:\\Users\\Asif Computer\\anaconda3\\envs\\conda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model with BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document using PyPDFLoader\n",
    "loaders = PyPDFLoader(\"AI_Engineer_Roadmap.pdf\")\n",
    "\n",
    "# Extract pages from the loaded PDF\n",
    "pages = loaders.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'AI_Engineer_Roadmap.pdf', 'page': 9}, page_content=' \\n   \\ncodebasics.io  \\n \\n10 \\no Track B (Affordable Fees):  \\n‚ñ™ Included in  the above Master Machine Learning for Data Science & AI  \\n \\nWeek 23, 24: Machine Learning Projects  with Deployment       \\n \\n‚Ä¢ You need to finish two end to end ML projects. One on Regression , the other on \\nClassification  \\n‚Ä¢ Regression Project: Bangalore property price prediction  \\no YouTube playlist link: https://bit.ly/3ivycWr  \\no Project covers following  \\n‚ñ™ Data cleaning  \\n‚ñ™ Feature engineering  \\n‚ñ™ Model building and hyper parameter tuning  \\n‚ñ™ Write flask server as a web backend  \\n‚ñ™ Building website for price prediction  \\n‚ñ™ Deployment to AWS  \\n‚Ä¢ Classification Project: Sports celebrity image classification  \\no YouTube playlist link: https://bit.ly/3ioaMSU  \\no Project covers following  \\n‚ñ™ Data collection and data cleaning  \\n‚ñ™ Feature engineering and model training  \\n‚ñ™ Flask server as a web backend  \\n‚ñ™ Building website and deployment  \\n‚Ä¢ ATS Resume Preparation  \\no Resumes are dying but not dead yet. Focus more on online presence.  \\no Here is the resume tips video along with some templates you can use for your \\ndata analyst resume: https://www.youtube.com/watch?v=buQSI8NLOMw  \\no Use this checklist to ensure you have the right ATS Resume:  Check here.  \\n \\n \\n‚Ä¢ Portfolio Building Resources:  \\nYou need a portfolio website in 2024. You can build your portfolio by using these free \\nresources.  \\n‚Ä¢ GitHub  \\no Upload your projects with code on github and using github.io create a \\nportfolio website  \\no Sample portfolio website: http://rajag0pal.github.io/  \\n ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store processed document chunks\n",
    "doc_list = []\n",
    "\n",
    "# Iterate over each page in the extracted pages\n",
    "for page in pages:\n",
    "    # Split the page content into smaller chunks\n",
    "    pg_split = text_splitter.split_text(page.page_content)\n",
    "\n",
    "    # Iterate over each chunk and create Document objects\n",
    "    for pg_sub_split in pg_split:\n",
    "        # Metadata for each chunk, including source and page number\n",
    "        metadata = {\"source\": \"AI policy\", \"page_no\": page.metadata[\"page\"] + 1}\n",
    "\n",
    "        # Create a Document object with content and metadata\n",
    "        doc_string = Document(page_content=pg_sub_split, metadata=metadata)\n",
    "\n",
    "        # Append the Document object to the list\n",
    "        doc_list.append(doc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=\"\"\n",
    "index_name=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore as lang_pinecone\n",
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents into vectors using LangPinecone\n",
    "vector = lang_pinecone.from_documents(\n",
    "    doc_list,                # List of Document objects to be converted into vectors\n",
    "    embed_model,             # Embedding model used for generating vector representations\n",
    "    index_name=index_name    # Name of the Pinecone index where vectors will be stored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Classification?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ss = vector.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codebasics.io  \\n \\n12 \\nWeek 28, 29, 30 : NLP or Computer Vision  & GenAI  üìÉ \\n \\n‚Ä¢ Many AI engineers  choose a specialized track which is either NLP or Computer vision. \\nYou don‚Äôt need to learn both.  \\n‚Ä¢ Natural Language Processing (N LP) \\no Topics  \\n‚ñ™ Regex  \\n‚ñ™ Text presentation: Count vectorizer, TF -IDF, BOW, Word2Vec, \\nEmbeddings  \\n‚ñ™ Text classification: Na√Øve Bayes  \\n‚ñ™ Fundamentals of Spacy & NLTP library  \\n‚ñ™ One end to end project  \\no Learning Resources  \\n‚ñ™ NLP YouTube playlist: https://bit.ly/3XnjfEZ  \\n \\n‚Ä¢ Comput er Vision (CV)  \\no Topics  \\n‚ñ™ Basic image processing techniques: Filtering, Edge Detection, Image \\nScaling, Rotation  \\n‚ñ™ Library to use: OpenCV  \\n‚ñ™ Convolutional Neural Networks (CNN) ‚Äì Already covered in deep \\nlearning.  \\n‚ñ™ Data preprocessing, augmentation ‚Äì Already covered in deep learning.  \\n‚Ä¢ Assignment  \\n‚òê NLP Track: Complete exercises in this playlist: https://bit.ly/3XnjfEZ  \\n \\nWeek 31, 32 : LLM & Langchain üìÉ \\n \\n‚Ä¢ Topics  \\no What is LLM, Vector database, Embeddings?  \\no RAG (Retrieval Augmented Generation)  \\no Langchain framework  \\n‚Ä¢ Learning Resources  \\no Langchain, LLM playlist: https://bit.ly/3RYpxuw'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks of Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codebasics.io  \\n \\n11 \\n‚Ä¢ Linktree  \\no Helpful to add multiple links in one page.  \\n \\n‚Ä¢ Assignment  \\no In above two projects make following changes  \\n‚òê Use FastAPI  instead of flask . FastAPI tutorial: https://youtu.be/Wr1JjhTt1Xg  \\n‚òê Regression project : Instead of property prediction, take any other project \\nof your interest from Kaggle for regress ion \\n‚òê Classification project : Instead of sports celebrity classification, take any \\nother project of your interest from Kaggle for classification and build end to \\nend solution along with deployment to AWS or Azure  \\n     ‚òê Add a link of your projects in your resume and LinkedIn.  \\n(Tag Codebasics, Dhaval Patel and Hemanand Vadivel with the hashtag \\n#dsroadmap24 so we can engage to increase your visibility)  \\n \\n \\nWeek 25, 26, 27 : Deep Learning           \\n \\n‚Ä¢ Topics  \\no What is a neural network? Forward propagation, back propagation  \\no Building multilayer perceptron  \\no Special neural network architectures  \\n‚ñ™ Convolutional neural network (CNN)  \\n‚ñ™ Sequence models: RNN, LSTM  \\n \\n‚Ä¢ Learning Resources  \\no Deep Learning playlist  (tensorflow) : https://bit.ly/3vOZ3zV  \\no Deep learning playlist (pytorch): https://bit.ly/3TzDbWp  \\no End to end potato disease  classification project: https://bit.ly/3QzkVJi  \\n \\n‚Ä¢ Assignment  \\n‚òê Instead of potato plant images use tomato plant images or some other image \\nclassification dataset . \\n‚òê Deploy to Azure instead of GCP .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in doc_list]\n",
    "svm_retriever = SVMRetriever.from_texts(texts, embed_model)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asif Computer\\AppData\\Local\\Temp\\ipykernel_8284\\4134095039.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs_svm=svm_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='codebasics.io  \\n \\n8 \\nWeek 17: Exploratory Data Analysis (EDA)           \\n \\n‚Ä¢ Exploratory Data Analysis (EDA)  \\no https://www.kaggle.com/code?searchQuery=exploratory+data+analysis  \\no Use the above link to search for exploratory data analysis notebooks.  \\no Practice EDA using at least 3 datasets.  \\n‚ñ™ e.g. https://www.kaggle.com/datasets/rishabhkarn/ipl -auction -\\n2023/data  \\n \\n‚Ä¢ Assignment  \\n‚òê Perform EDA (Exploratory data analysis on at least 2 additional datasets  on \\nKaggle)  \\n \\nWeek 18, 19, 20, 21 : Machine Learning          \\n \\n‚Ä¢ Machine Learning : Preprocessing  \\no Handling NA values , outlier treatment, data normalization  \\no One hot encoding, label encoding  \\no Feature engineering  \\no Train test split  \\no Cross validation  \\n‚Ä¢ Machine Learning: Model  Building  \\no Types of ML: Supervised, Unsupervised  \\no Supervised: Regression vs Classification  \\no Linear models  \\n‚ñ™ Linear regression, logistic regression  \\n‚ñ™ Gradient descent  \\no Nonlinear models ( tree-based  models)  \\n‚ñ™ Decision tree  \\n‚ñ™ Random forest  \\n‚ñ™ XGBoost  \\no Model evaluation  \\n‚ñ™ Regression: Mean Squared Error, Mean Absolute Error, MAPE  \\n‚ñ™ Classification: Accuracy, Precision -Recall, F1 Score, ROC Curve, \\nConfusion matrix  \\no Hyperparameter tunning: GridSearchCV, RandomSearchCV')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"machine learning?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='codebasics.io  \\n \\n11 \\n‚Ä¢ Linktree  \\no Helpful to add multiple links in one page.  \\n \\n‚Ä¢ Assignment  \\no In above two projects make following changes  \\n‚òê Use FastAPI  instead of flask . FastAPI tutorial: https://youtu.be/Wr1JjhTt1Xg  \\n‚òê Regression project : Instead of property prediction, take any other project \\nof your interest from Kaggle for regress ion \\n‚òê Classification project : Instead of sports celebrity classification, take any \\nother project of your interest from Kaggle for classification and build end to \\nend solution along with deployment to AWS or Azure  \\n     ‚òê Add a link of your projects in your resume and LinkedIn.  \\n(Tag Codebasics, Dhaval Patel and Hemanand Vadivel with the hashtag \\n#dsroadmap24 so we can engage to increase your visibility)  \\n \\n \\nWeek 25, 26, 27 : Deep Learning           \\n \\n‚Ä¢ Topics  \\no What is a neural network? Forward propagation, back propagation  \\no Building multilayer perceptron  \\no Special neural network architectures  \\n‚ñ™ Convolutional neural network (CNN)  \\n‚ñ™ Sequence models: RNN, LSTM  \\n \\n‚Ä¢ Learning Resources  \\no Deep Learning playlist  (tensorflow) : https://bit.ly/3vOZ3zV  \\no Deep learning playlist (pytorch): https://bit.ly/3TzDbWp  \\no End to end potato disease  classification project: https://bit.ly/3QzkVJi  \\n \\n‚Ä¢ Assignment  \\n‚òê Instead of potato plant images use tomato plant images or some other image \\nclassification dataset . \\n‚òê Deploy to Azure instead of GCP .')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"deep learning?\"\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
