{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # RAG-based AI assistant for scientific paper summarization and suggestion\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:1200/0*ffG7IPkdztO6BARk.png)"
      ],
      "metadata": {
        "id": "w8D13dv6jNkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview\n",
        "This project builds a smart PDF question-answering system using:\n",
        "\n",
        "- LangChain for chaining components\n",
        "- FAISS for vector-based document retrieval\n",
        "- SentenceTransformers for document embedding\n",
        "- ChatGroq (Qwen-QWQ-32B) as the LLM\n",
        "- LangChain PromptTemplate for custom response formatting\n",
        "\n",
        "The system allows users to ask questions about the content of a scientific paper (PDF), and get concise, accurate, and technical answers based on the document."
      ],
      "metadata": {
        "id": "-SJccGUPjPFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Structure\n",
        "\n",
        "```bash\n",
        "scientific_paper_summarization_and_suggestion\n",
        "â”œâ”€â”€ data/paper.pdf               # Your input scientific paper\n",
        "â”œâ”€â”€ notebook/notebook.ipynb           # Your notebook\n",
        "â”œâ”€â”€ vector_index/           # FAISS index folder (auto-generated)\n",
        "â”œâ”€â”€ .env                    # Your API key\n",
        "â””â”€â”€ main.py                 # Main project script\n",
        "```"
      ],
      "metadata": {
        "id": "kC-lor_vjS2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Code Explanation\n",
        "### 1. Environment Setup\n",
        "\n"
      ],
      "metadata": {
        "id": "iqK8qdLWjvc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain_community langchain_groq pypdf huggingface faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RF2lOrBlNXq",
        "outputId": "a562b0a2-0287-45dc-8484-85b2ee0089bd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.6.1)\n",
            "Requirement already satisfied: huggingface in /usr/local/lib/python3.11/dist-packages (0.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: groq<1,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.28.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (4.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.28.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.28.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "I-rI6DhTjIM-"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads your GROQ API key from a .env file.\n",
        "\n",
        "Always keep your keys secret and never hardcode them.\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:1400/1*hcvunNJ4IolhZ6Qav5UPjQ.png)\n"
      ],
      "metadata": {
        "id": "EbDlbvSZj2Mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. PDF Loading and Chunking\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-UpVGWWNj3Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def load_and_split_pdf(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    return splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "5uGgymRyj_GB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the PDF file using PyPDFLoader.\n",
        "\n",
        "Splits the PDF into smaller chunks using RecursiveCharacterTextSplitter to make them fit LLM context windows."
      ],
      "metadata": {
        "id": "rYCPULE8kA4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Embedding and Creating a FAISS Vector Index\n"
      ],
      "metadata": {
        "id": "rw5GEt2rkDTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def create_vector_store(docs, index_path=\"vector_index\"):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "    vectorstore.save_local(index_path)\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "-b83jNu-kD3t"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Converts text chunks into vectors using a HuggingFace model.<br>\n",
        "Saves vectors using FAISS to enable fast similarity search."
      ],
      "metadata": {
        "id": "BrbWxngnkFiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.  Load an Existing Vector Store\n"
      ],
      "metadata": {
        "id": "sWbP4inXkLWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def load_vector_store(index_path=\"vector_index\"):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "dfNnbw6AkLvV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reloads a previously created FAISS vector index from disk.\n"
      ],
      "metadata": {
        "id": "l9jiUcXikMGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Custom Prompt Template\n"
      ],
      "metadata": {
        "id": "5oowztQpkQhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "CUSTOM_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are a scientific research assistant.\n",
        "Given the following context from scientific papers, answer the user's question.\n",
        "Be concise, technical, and include any relevant research suggestions.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "_FbvDE7TkQwH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines how the LLM should respond.\n",
        "\n",
        "It provides role-based instructions and formats the prompt cleanly."
      ],
      "metadata": {
        "id": "FJZdia3vkQ-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Create Retrieval-Based QA Chain\n"
      ],
      "metadata": {
        "id": "5dVWh9AVkgAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "def create_qa_chain(vectorstore):\n",
        "    llm = ChatGroq(model_name=\"qwen-qwq-32b\", temperature=0, api_key=api_key)\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": CUSTOM_PROMPT}\n",
        "    )\n",
        "    return qa_chain"
      ],
      "metadata": {
        "id": "uPpstntfkgZ3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes the Qwen-32B model from Groq.\n",
        "\n",
        "Uses RetrievalQA to combine vector-based context retrieval with the LLM.\n",
        "\n",
        "chain_type=\"stuff\" puts all retrieved documents directly into the prompt."
      ],
      "metadata": {
        "id": "LcZnhfIgkkU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Main Function: Build or Load Vector Store & Run Query\n"
      ],
      "metadata": {
        "id": "g_gCrPqokl_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/paper.pdf\"\n",
        "user_query = \"Summarize the main findings of the paper.\"\n",
        "\n",
        "docs = load_and_split_pdf(pdf_path)\n",
        "vectorstore = create_vector_store(docs)\n",
        "vectorstore = load_vector_store()\n",
        "\n",
        "qa_chain = create_qa_chain(vectorstore)\n",
        "response = qa_chain.run(user_query)\n",
        "print(\"\\nðŸ’¡ AI Response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sxCiVSqkmjw",
        "outputId": "d9f74c32-19bf-49d6-d037-8042b75812b6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¡ AI Response:\n",
            " \n",
            "<think>\n",
            "Okay, I need to summarize the main findings of the paper based on the provided context. Let me look at the information given. The user provided references from the paper's bibliography and a general procedure for a chemical reaction. The references include works from Takeuchi, Bailey, and others, mostly from the 90s and late 90s. The general procedure describes a chemical synthesis method: they're mixing compounds like 3a, NBu4Br, NaH, and ethyl bromofluoroacetate in THF, then adding NH4Cl afterward.\n",
            "\n",
            "Hmm, the question is asking for the main findings, but the context given doesn't explicitly state the results or conclusions. The references might hint at the paper's focus. Takeuchi's work in 1997 and 1998 could be related to organic synthesis, maybe of some compounds like antibiotics or pharmaceuticals, given the journal names (J. Antibiot., Chem. Pharm. Bull.). The procedure outlined is a synthesis step, perhaps for creating a specific compound. Since the procedure uses fluorinated reagents (ethyl bromofluoroacetate), maybe the paper is about synthesizing fluorinated compounds, which are common in pharmaceuticals.\n",
            "\n",
            "The main findings might involve the successful synthesis of a particular compound using this method, possibly with high yield or under certain conditions. The references to other works by the same authors might indicate that this is part of a series, so the current paper could be optimizing a reaction or reporting a new compound. Since the procedure uses sodium hydride and quaternary ammonium salts (NBu4Br), it might be a nucleophilic substitution or alkylation reaction. The key here is that without the full paper, I have to infer based on the procedure and references.\n",
            "\n",
            "The user wants a concise, technical summary. Since the procedure is a general method, maybe the main finding is the development of a new synthetic route for a specific class of compounds, perhaps with improved efficiency or novel conditions. The references to J. Antibiot. suggest the target compound could be an antibiotic. Alternatively, the study might be about the reaction conditions leading to a desired product with specific functional groups, like fluorine, which is highlighted here.\n",
            "\n",
            "I should also consider that the answer needs to include any relevant research suggestions. Since the procedure is part of the paper's methodology, the main findings would be the successful synthesis using this method, possibly with characterization data not shown here. The research suggestions might involve testing the synthesized compounds for biological activity, optimizing reaction parameters, or exploring other substrates.\n",
            "\n",
            "Wait, the user's context includes a list of references and a specific experimental procedure. The main findings are likely tied to the synthesis method described. The paper probably reports a new method for synthesizing a compound (maybe a fluorinated derivative) using the outlined steps. The key points would be the reaction conditions, reagents used, and the outcome (like product formation, yield, etc.). Since the procedure ends with adding NH4Cl to stop the reaction, maybe the product is isolated and characterized, leading to the main result.\n",
            "\n",
            "So, putting it together: The paper's main findings are the successful synthesis of a compound (probably a fluorinated derivative) via a nucleophilic substitution reaction using ethyl bromofluoroacetate under specific conditions. The method uses NaH and NBu4Br as additives in THF, leading to the desired product. The references suggest prior work on similar compounds, so this might be an extension or optimization. The main result is the synthesis protocol and possibly the characterization of the product.\n",
            "\n",
            "For research suggestions, maybe testing the generality of the reaction with other substrates, evaluating reaction kinetics, or assessing the biological activity of the synthesized compounds, especially if they're antibiotics as per the journal references.\n",
            "</think>\n",
            "\n",
            "The paper describes a synthetic method for preparing fluorinated compounds via nucleophilic substitution, utilizing ethyl bromofluoroacetate under specific conditions. The key steps involve reacting substrate **3a** with NaH, NBuâ‚„Br, and ethyl bromofluoroacetate in THF, followed by quenching with NHâ‚„Cl. The main finding is the successful synthesis of a fluorinated product under these conditions, likely optimizing reaction parameters (e.g., stoichiometry, temperature, reagent choice). The method contributes to the synthesis of bioactive molecules, possibly antibiotics or pharmaceuticals, as indicated by cited literature (e.g., references to *J. Antibiot.*, *Chem. Pharm. Bull.*).  \n",
            "\n",
            "**Research suggestions**:  \n",
            "1. Investigate the scope of this reaction with other substrates or electrophiles to assess generality.  \n",
            "2. Characterize the product(s) via NMR, MS, or X-ray crystallography for structural confirmation.  \n",
            "3. Evaluate biological activity (e.g., antimicrobial properties) of synthesized fluorinated compounds, given the context of cited antibiotic studies.  \n",
            "4. Optimize reaction conditions (e.g., temperature, solvent, base) to improve yield or stereoselectivity.  \n",
            "\n",
            "The study advances fluorination methodologies, critical for drug development, by demonstrating a viable synthetic pathway under mild conditions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Workflow:\n",
        "![Alt text](https://s3.amazonaws.com/samples.clarifai.com/rag_template-image1.webp)\n",
        "\n",
        "Check if a vector index already exists.\n",
        "\n",
        "If not, load the PDF â†’ chunk â†’ embed â†’ save FAISS index.\n",
        "\n",
        "Create a QA chain with retrieval + Groq LLM.\n",
        "\n",
        "Ask the user query and print the response.\n"
      ],
      "metadata": {
        "id": "PDCEUP28k2al"
      }
    }
  ]
}